def parse_type_func_param() -> &Typespec {
  let type = parse_type();
  if (match_token(TokenKind.Colon)) {
    if (type.kind != Typespec.Name) {
      error(
        token.pos,
        "Colons in parameters of function types must be preceded by names.",
      );
    }
    type = parse_type();
  }
  return type;
}

def parse_type_func() -> &Typespec {
  let pos = token.pos;
  let args: *&Typespec = NULL;
  let has_varargs: bool = false;
  expect_token(TokenKind.Lparen);
  while (!is_token(TokenKind.Rparen)) {
    if (match_token(TokenKind.Ellipsis)) {
      if (has_varargs) {
        error(token.pos, "Multiple ellipsis instances in function type");
      }
      has_varargs = true;
    } else {
      if (has_varargs) {
        error(token.pos, "Ellipsis must be last parameter in function type");
      }
      let param = parse_type_func_param();
      buf_push(&args as &&void, &param, sizeof(param));
    }
    if (!match_token(TokenKind.Comma)) {
      break;
    }
  } while;
  expect_token(TokenKind.Rparen);
  let ret: &Typespec = NULL;
  if (match_token(TokenKind.Arrow)) {
    ret = parse_type();
  }
  return new_typespec_func(pos, (args, buf_len(args)), ret, has_varargs);
}

def parse_type_args() -> Slice<TypeArg> {
  let args: *TypeArg;
  if (match_token(TokenKind.Lt)) {
    do {
      let type_arg = TypeArg{pos: token.pos, val: parse_type()};
      buf_push(&args as &&void, &type_arg, sizeof(*args));
    } while (match_token(TokenKind.Comma));
    if (is_token(TokenKind.Rshift)) {
      // Abusive change of current token.
      token.kind = TokenKind.Gt;
    } else {
      expect_token(TokenKind.Gt);
    }
  }
  return {items: args, length: buf_len(args)};
}

def parse_type_base() -> &Typespec {
  if (is_token(TokenKind.Name)) {
    let pos = token.pos;
    let names: *TypespecName;
    let token_name = token.name;
    next_token();
    let first_name =
      TypespecName{name: token_name, type_args: parse_type_args()};
    buf_push(&names as &&void, &first_name, sizeof(first_name));
    while (match_token(TokenKind.Dot)) {
      let name =
        TypespecName{name: parse_name(), type_args: parse_type_args()};
      buf_push(&names as &&void, &name, sizeof(name));
    }
    let result = new_typespec_name(pos, (names, buf_len(names)));
    buf_free(&names as &&void);
    return result;
  } else if (match_keyword(do_keyword)) {
    return parse_type_func();
  } else if (match_token(TokenKind.Lparen)) {
    let type = parse_type();
    expect_token(TokenKind.Rparen);
    return type;
  } else {
    fatal_error(token.pos, "Unexpected token %s in type", token_info());
    return NULL;
  }
}

def parse_type() -> &Typespec {
  let pos = token.pos;
  let type: &Typespec;
  // Check modifiers.
  if (match_token(TokenKind.Lbracket)) {
    let size: &Expr = NULL;
    if (!is_token(TokenKind.Rbracket)) {
      size = parse_expr();
    }
    expect_token(TokenKind.Rbracket);
    type = new_typespec_array(pos, parse_type(), size);
  } else if (match_token(TokenKind.And)) {
    type = new_typespec_ref(pos, parse_type(), false);
  } else if (match_token(TokenKind.AndAnd)) {
    // Special double ref handling for &&, so the tokenizer can stay consistent.
    type = new_typespec_ref(pos, parse_type(), false);
    type = new_typespec_ref(pos, type, false);
  } else if (match_token(TokenKind.Mul)) {
    type = new_typespec_ptr(pos, parse_type(), false);
  } else if (match_keyword(const_keyword)) {
    type = new_typespec_const(pos, parse_type());
  } else if (match_keyword(own_keyword)) {
    // TODO Track owning.
    type = parse_type();
  } else {
    type = parse_type_base();
  }
  return type;
}

def parse_expr_struct_field(is_tuple: bool) -> CompoundField {
  let pos = token.pos;
  let expr = parse_expr();
  if (match_token(TokenKind.Colon)) {
    if (expr.kind != Expr.Name) {
      fatal_error(token.pos, "Named initializer in compound literal must be preceded by field name");
    }
    return (CompoundField.Name, pos, parse_expr(), name: expr.name);
  } else {
    if (!is_tuple) {
      warning(pos, "Keyless field in struct literal");
    }
    return (CompoundField.Default, pos, expr);
  }
}

def parse_expr_struct(type: &Typespec) -> &Expr {
  let pos = token.pos;
  expect_token(TokenKind.Lbrace);
  let fields: *CompoundField;
  while (!is_token(TokenKind.Rbrace)) {
    let field = parse_expr_struct_field(false);
    buf_push(&fields as &&void, &field, sizeof(field));
    if (!match_token(TokenKind.Comma)) {
      break;
    }
  }
  expect_token(TokenKind.Rbrace);
  return new_expr_compound(
    pos, ExprCompound.Struct, type, (fields, buf_len(fields)),
  );
}

def parse_expr_tuple(type: &Typespec) -> &Expr {
  let pos = token.pos;
  expect_token(TokenKind.Lparen);
  let fields: *CompoundField;
  while (!is_token(TokenKind.Rparen)) {
    let field = parse_expr_struct_field(true);
    if (
      !fields &&
      field.kind == CompoundField.Default &&
      match_token(TokenKind.Rparen)
    ) {
      // First field had nothing special, so just parens.
      return new_expr_paren(pos, field.val);
    }
    buf_push(&fields as &&void, &field, sizeof(field));
    if (!match_token(TokenKind.Comma)) {
      break;
    }
  }
  expect_token(TokenKind.Rparen);
  return new_expr_compound(
    pos, ExprCompound.Tuple, type, (fields, buf_len(fields)),
  );
}

def parse_expr_list_item() -> CompoundField {
  let pos = token.pos;
  let expr = parse_expr();
  if (match_token(TokenKind.Colon)) {
    switch (expr.kind) {
      Compound => {
        if (expr.compound.kind == ExprCompound.List) {
          // Allow square brackets around indices, to match structs and js.
          // TODO Really?
          let fields = expr.compound.fields;
          if (fields.length != 1) {
            fatal_error(token.pos, "Computed field key must be single");
          }
          warning(pos, "Needless brackets");
          expr = fields[0].val;
        }
      }
      Name => {
        // For now, list literals and tuple literals are the same.
        // TODO Probably change tuples to () at some point.
        return (
          CompoundField.Name, pos, parse_expr(), name: expr.name,
        );
      }
    }
    // Any other errors in expression type can be resolved later.
    return (CompoundField.Index, pos, parse_expr(), index: expr);
  } else {
    return (CompoundField.Default, pos, expr);
  }
}

def parse_expr_list(type: &Typespec) -> &Expr {
  let pos = token.pos;
  expect_token(TokenKind.Lbracket);
  let fields: *CompoundField = NULL;
  while (!is_token(TokenKind.Rbracket)) {
    let field = parse_expr_list_item();
    buf_push(&fields as &&void, &field, sizeof(field));
    if (!match_token(TokenKind.Comma)) {
      break;
    }
  }
  expect_token(TokenKind.Rbracket);
  return new_expr_compound(
    pos, ExprCompound.List, type, (fields, buf_len(fields)),
  );
}

def parse_expr_operand() -> &Expr {
  let pos = token.pos;
  if (is_token(TokenKind.Int)) {
    let val = token.int_val;
    let mod = token.mod;
    let suffix = token.suffix;
    next_token();
    return new_expr_int(pos, val, mod, suffix);
  } else if (is_token(TokenKind.Float)) {
    let start = token.start;
    let end = token.end;
    let val = token.float_val;
    let suffix = token.suffix;
    next_token();
    return new_expr_float(pos, start, end, val, suffix);
  } else if (is_token(TokenKind.Str)) {
    let val = token.str_val;
    let mod = token.mod;
    next_token();
    return new_expr_str(pos, val, mod);
  } else if (is_token(TokenKind.Name)) {
    let name = token.name;
    next_token();
    if (is_token(TokenKind.Lbrace)) {
      return parse_expr_struct(new_typespec_name1(pos, name));
    } else {
      return new_expr_name(pos, name);
    }
  } else if (match_keyword(sizeof_keyword)) {
    if (match_token(TokenKind.Lparen)) {
      let expr = parse_expr();
      expect_token(TokenKind.Rparen);
      return new_expr_sizeof_expr(pos, expr);
    } else {
      expect_token(TokenKind.Lt);
      let type = parse_type();
      expect_close_angle();
      return new_expr_sizeof_type(pos, type);
    }
  } else if (match_keyword(alignof_keyword)) {
    if (match_token(TokenKind.Lparen)) {
      let expr = parse_expr();
      expect_token(TokenKind.Rparen);
      return new_expr_alignof_expr(pos, expr);
    } else {
      expect_token(TokenKind.Lt);
      let type = parse_type();
      expect_close_angle();
      return new_expr_alignof_type(pos, type);
    }
  } else if (match_keyword(typeof_keyword)) {
    if (match_token(TokenKind.Lparen)) {
      let expr = parse_expr();
      expect_token(TokenKind.Rparen);
      return new_expr_typeof_expr(pos, expr);
    } else {
      expect_token(TokenKind.Lt);
      let type = parse_type();
      expect_close_angle();
      return new_expr_typeof_type(pos, type);
    }
  } else if (match_keyword(offsetof_keyword)) {
    expect_token(TokenKind.Lparen);
    let type = parse_type();
    expect_token(TokenKind.Comma);
    let name = parse_name();
    expect_token(TokenKind.Rparen);
    return new_expr_offsetof(pos, type, name);
  } else if (is_token(TokenKind.Lbracket)) {
    return parse_expr_list(NULL);
  } else if (is_token(TokenKind.Lbrace)) {
    return parse_expr_struct(NULL);
  } else if (match_token(TokenKind.Lt)) {
    let type = parse_type();
    expect_close_angle();
    if (is_token(TokenKind.Lbrace)) {
      return parse_expr_struct(type);
    } else if (is_token(TokenKind.Lparen)) {
      return parse_expr_tuple(type);
    } else if (is_token(TokenKind.Lbracket)) {
      return parse_expr_list(type);
    } else {
      return new_expr_cast(pos, type, parse_expr_unary());
    }
  } else if (is_token(TokenKind.Lparen)) {
    return parse_expr_tuple(NULL);
  } else {
    fatal_error(token.pos, "Unexpected token %s in expression", token_info());
    return NULL;
  }
}

def parse_expr_base() -> &Expr {
  let expr = parse_expr_operand();
  while (is_token(TokenKind.Lparen) || is_token(TokenKind.Lbracket) || is_token(TokenKind.Dot) || is_token(TokenKind.Inc) || is_token(TokenKind.Dec)) {
    let pos = token.pos;
    if (match_token(TokenKind.Lparen)) {
      let args: *CompoundField;
      // TODO Named args for the sake of tuples, at least.
      while (!is_token(TokenKind.Rparen)) {
        let arg = parse_expr_struct_field(true);
        buf_push(&args as &&void, &arg, sizeof(arg));
        if (!match_token(TokenKind.Comma)) {
          break;
        }
      }
      expect_token(TokenKind.Rparen);
      expr = new_expr_call(pos, expr, (args, buf_len(args)));
    } else if (match_token(TokenKind.Lbracket)) {
      let index = parse_expr();
      expect_token(TokenKind.Rbracket);
      expr = new_expr_index(pos, expr, index);
    } else if (is_token(TokenKind.Dot)) {
      next_token();
      if (is_token(TokenKind.Lt)) {
        // Generic type args.
        let type_args = parse_type_args();
        switch (expr.kind) {
          Field => expr.field.type_args = type_args;
          Name => expr.type_args = type_args;
          default => {
            fatal_error(pos, "Unexpected type args");
          }
        }
      } else {
        // Field access.
        let field = token.name;
        expect_token(TokenKind.Name);
        expr = new_expr_field(pos, expr, field);
      }
    } else {
      #assert(is_token(TokenKind.Inc) || is_token(TokenKind.Dec));
      let op = token.kind;
      next_token();
      expr = new_expr_modify(pos, op, true, expr);
    }
  } while;
  return expr;
}

def is_unary_op() -> bool {
  return
    is_token(TokenKind.Add) ||
    is_token(TokenKind.Sub) ||
    is_token(TokenKind.Mul) ||
    is_token(TokenKind.And) ||
    is_token(TokenKind.Neg) ||
    is_token(TokenKind.Not) ||
    is_token(TokenKind.Inc) ||
    is_token(TokenKind.Dec);
}

def parse_expr_unary() -> &Expr {
  if (is_unary_op()) {
    let pos = token.pos;
    let op = token.kind;
    next_token();
    if (op == TokenKind.Inc || op == TokenKind.Dec) {
      return new_expr_modify(pos, op, false, parse_expr_unary());
    } else {
      return new_expr_unary(pos, op, parse_expr_unary());
    }
  } else {
    return parse_expr_base();
  }
}

def is_mul_op() -> bool {
  return TokenKind.FirstMul <= token.kind && token.kind <= TokenKind.LastMul;
}

def parse_expr_as() -> &Expr {
  let expr = parse_expr_unary();
  while (is_token_name(as_name)) {
    let pos = token.pos;
    next_token();
    let type = parse_type();
    expr = new_expr_cast(pos, type, expr);
  }
  return expr;
}

def parse_expr_mul() -> &Expr {
  let expr = parse_expr_as();
  while (is_mul_op()) {
    let pos = token.pos;
    let op = token.kind;
    next_token();
    expr = new_expr_binary(pos, op, expr, parse_expr_as());
  }
  return expr;
}

def is_add_op() -> bool {
  return TokenKind.FirstAdd <= token.kind && token.kind <= TokenKind.LastAdd;
}

def parse_expr_add() -> &Expr {
  let expr = parse_expr_mul();
  while (is_add_op()) {
    let pos = token.pos;
    let op = token.kind;
    next_token();
    expr = new_expr_binary(pos, op, expr, parse_expr_mul());
  }
  return expr;
}

def is_cmp_op() -> bool {
  return TokenKind.FirstCmp <= token.kind && token.kind <= TokenKind.LastCmp;
}

def parse_expr_cmp() -> &Expr {
  let expr = parse_expr_add();
  while (is_cmp_op()) {
    let pos = token.pos;
    let op = token.kind;
    next_token();
    expr = new_expr_binary(pos, op, expr, parse_expr_add());
  }
  return expr;
}

def parse_expr_and() -> &Expr {
  let expr = parse_expr_cmp();
  while (match_token(TokenKind.AndAnd)) {
    let pos = token.pos;
    expr = new_expr_binary(pos, TokenKind.AndAnd, expr, parse_expr_cmp());
  }
  return expr;
}

def parse_expr_or() -> &Expr {
  let expr = parse_expr_and();
  while (match_token(TokenKind.OrOr)) {
    let pos = token.pos;
    expr = new_expr_binary(pos, TokenKind.OrOr, expr, parse_expr_and());
  }
  return expr;
}

def parse_expr_ternary() -> &Expr {
  let pos = token.pos;
  let expr = parse_expr_or();
  if (match_token(TokenKind.Question)) {
    let then_expr = parse_expr_ternary();
    expect_token(TokenKind.Colon);
    let else_expr = parse_expr_ternary();
    expr = new_expr_ternary(pos, expr, then_expr, else_expr);
  }
  return expr;
}

def parse_expr() -> &Expr {
  return parse_expr_ternary();
}

def parse_paren_expr() -> &Expr {
  expect_token(TokenKind.Lparen);
  let expr = parse_expr();
  expect_token(TokenKind.Rparen);
  return expr;
}

def parse_stmt_block() -> StmtList {
  let pos = token.pos;
  expect_token(TokenKind.Lbrace);
  let stmts: *&Stmt = NULL;
  while (!is_token_eof() && !is_token(TokenKind.Rbrace)) {
    let stmt = parse_stmt();
    buf_push(&stmts as &&void, &stmt, sizeof(stmt));
  }
  expect_token(TokenKind.Rbrace);
  return new_stmt_list(pos, (stmts, buf_len(stmts)));
}

def parse_stmt_if(pos: SrcPos) -> &Stmt {
  expect_token(TokenKind.Lparen);
  let cond: &Expr;
  let init: &Stmt;
  if (match_keyword(let_keyword)) {
    init = parse_let_stmt(pos);
    if (match_token(TokenKind.Semicolon)) {
      cond = parse_expr();
    }
  } else {
    cond = parse_expr();
  }
  expect_token(TokenKind.Rparen);
  let then_block: StmtList = parse_stmt_block();
  let else_block: StmtList;
  let elseifs: *ElseIf;
  while (match_keyword(else_keyword)) {
    if (!match_keyword(if_keyword)) {
      else_block = parse_stmt_block();
      break;
    }
    let elseif_cond: &Expr = parse_paren_expr();
    let elseif_block: StmtList = parse_stmt_block();
    let elseif = ElseIf(elseif_cond, elseif_block);
    buf_push(&elseifs as &&void, &elseif, sizeof(elseif));
  } while;
  return new_stmt_if(
    pos, init, cond, then_block, (elseifs, buf_len(elseifs)), else_block,
  );
}

def parse_stmt_while(pos: SrcPos) -> &Stmt {
  let cond: &Expr = parse_paren_expr();
  return new_stmt_while(pos, cond, parse_stmt_block());
}

def parse_stmt_do_while(pos: SrcPos) -> &Stmt {
  let block: StmtList = parse_stmt_block();
  if (!match_keyword(while_keyword)) {
    fatal_error(token.pos, "Expected 'while' after 'do' block");
    return NULL;
  }
  let stmt: &Stmt = new_stmt_do_while(pos, parse_paren_expr(), block);
  expect_token(TokenKind.Semicolon);
  return stmt;
}

def is_assign_op() -> bool {
  return
    TokenKind.FirstAssign <= token.kind && token.kind <= TokenKind.LastAssign;
}

def parse_let_stmt(pos: SrcPos) -> &Stmt {
  let is_mut = match_keyword(mut_keyword);
  let name = parse_name();
  if (match_token(TokenKind.Assign)) {
    return new_stmt_init(pos, name, is_mut, NULL, parse_expr());
  } else if (match_token(TokenKind.Colon)) {
    let type = parse_type();
    let expr: &Expr;
    if (match_token(TokenKind.Assign)) {
      expr = parse_expr();
    } else {
      // For now, just let all later assignments slide.
      is_mut = true;
    }
    return new_stmt_init(pos, name, is_mut, type, expr);
  } else {
    fatal_error(token.pos, "Expected : or = after let, got %s", token_info());
    return NULL;
  }
}

def parse_simple_stmt() -> &Stmt {
  let pos = token.pos;
  let stmt: &Stmt;
  if (match_keyword(let_keyword)) {
    stmt = parse_let_stmt(pos);
  } else {
    let expr = parse_expr();
    if (expr.kind == Expr.Name && match_token(TokenKind.Colon)) {
      stmt = new_stmt_label(pos, expr.name);
    } else if (is_assign_op()) {
      let op = token.kind;
      next_token();
      stmt = new_stmt_assign(pos, op, expr, parse_expr());
    } else {
      stmt = new_stmt_expr(pos, expr);
    }
  }
  return stmt;
}

def parse_stmt_for_each(pos: SrcPos, get_ref: bool, expr: &Expr) -> &Stmt {
  let has_varargs: bool;
  let params: Slice<FuncParam>;
  if (match_keyword(do_keyword)) {
    params = parse_decl_func_params(&has_varargs, true);
  }
  // Check varargs up front, since we don't even want to bother to record it.
  if (has_varargs) {
    fatal_error(pos, "Varargs not allowed in for-each loop");
  }
  return new_stmt_for_each(pos, get_ref, expr, params, parse_stmt_block());
}

def parse_stmt_for(pos: SrcPos) -> &Stmt {
  let get_ref = match_token(TokenKind.And);
  expect_token(TokenKind.Lparen);
  let init: &Stmt = NULL;
  if (!is_token(TokenKind.Semicolon)) {
    // TODO "For in" loops before we get back to generics.
    init = parse_simple_stmt();
  }
  if (init && init.kind == Stmt.Expr && is_token(TokenKind.Rparen)) {
    next_token();
    return parse_stmt_for_each(pos, get_ref, init.expr);
  } else if (get_ref) {
    fatal_error(pos, "Ref requested for segmented for loop");
  }
  expect_token(TokenKind.Semicolon);
  let cond: &Expr = NULL;
  if (!is_token(TokenKind.Semicolon)) {
    cond = parse_expr();
  }
  let next: &Stmt = NULL;
  if (match_token(TokenKind.Semicolon)) {
    if (!is_token(TokenKind.Rparen)) {
      next = parse_simple_stmt();
      if (next.kind == Stmt.Init) {
        error(token.pos, "Init statements not allowed in for-statement's next clause");
      }
    }
  } if;
  expect_token(TokenKind.Rparen);
  return new_stmt_for(pos, init, cond, next, parse_stmt_block());
}

def parse_switch_case_pattern() -> SwitchCasePattern {
  if (match_keyword(default_keyword)) {
    return SwitchCasePattern {is_default: true};
  }
  let start = parse_expr();
  let end: &Expr;
  if (match_token(TokenKind.Ellipsis)) {
    end = parse_expr();
  }
  return (false, start, end);
}

def parse_stmt_switch_case() -> SwitchCase {
  let patterns: *SwitchCasePattern;
  let pattern = parse_switch_case_pattern();
  buf_push(&patterns as &&void, &pattern, sizeof(pattern));
  while (match_token(TokenKind.Comma)) {
    pattern = parse_switch_case_pattern();
    buf_push(&patterns as &&void, &pattern, sizeof(pattern));
  }
  expect_token(TokenKind.Spear);
  let pos = token.pos;
  let stmt = parse_stmt();
  return (
    // TODO Should the patterns be ast_dup_slice'd?
    (patterns, buf_len(patterns)), new_stmt_list(pos, (&stmt, 1)),
  );
}

def parse_stmt_switch(pos: SrcPos) -> &Stmt {
  let expr: &Expr = parse_paren_expr();
  let cases: *SwitchCase;
  expect_token(TokenKind.Lbrace);
  while (!is_token_eof() && !is_token(TokenKind.Rbrace)) {
    let case_stmt = parse_stmt_switch_case();
    buf_push(&cases as &&void, &case_stmt, sizeof(case_stmt));
  }
  expect_token(TokenKind.Rbrace);
  return new_stmt_switch(pos, expr, (cases, buf_len(cases)));
}

def parse_stmt() -> &Stmt {
  let notes = parse_notes();
  let pos = token.pos;
  let stmt: &Stmt;
  if (match_keyword(if_keyword)) {
    if (match_token(TokenKind.Semicolon)) {
      return new_stmt_close(pos, Stmt.If);
    }
    stmt = parse_stmt_if(pos);
  } else if (match_keyword(while_keyword)) {
    if (match_token(TokenKind.Semicolon)) {
      return new_stmt_close(pos, Stmt.While);
    }
    stmt = parse_stmt_while(pos);
  } else if (match_keyword(do_keyword)) {
    stmt = parse_stmt_do_while(pos);
  } else if (match_keyword(for_keyword)) {
    if (match_token(TokenKind.Semicolon)) {
      return new_stmt_close(pos, Stmt.For);
    }
    stmt = parse_stmt_for(pos);
  } else if (match_keyword(switch_keyword)) {
    if (match_token(TokenKind.Semicolon)) {
      return new_stmt_close(pos, Stmt.Switch);
    }
    stmt = parse_stmt_switch(pos);
  } else if (is_token(TokenKind.Lbrace)) {
    stmt = new_stmt_block(pos, parse_stmt_block());
  } else if (match_keyword(break_keyword)) {
    expect_token(TokenKind.Semicolon);
    stmt = new_stmt_break(pos);
  } else if (match_keyword(continue_keyword)) {
    expect_token(TokenKind.Semicolon);
    stmt = new_stmt_continue(pos);
  } else if (match_keyword(return_keyword)) {
    let expr: &Expr;
    if (!is_token(TokenKind.Semicolon)) {
      expr = parse_expr();
    }
    expect_token(TokenKind.Semicolon);
    stmt = new_stmt_return(pos, expr);
  } else if (match_token(TokenKind.Pound)) {
    let note: Note = parse_note();
    expect_token(TokenKind.Semicolon);
    stmt = new_stmt_note(pos, note);
  } else if (match_keyword(goto_keyword)) {
    stmt = new_stmt_goto(pos, parse_name());
    expect_token(TokenKind.Semicolon);
  } else {
    stmt = parse_simple_stmt();
    if (stmt.kind != Stmt.Label) {
      expect_token(TokenKind.Semicolon);
    }
  }
  stmt.notes = notes;
  return stmt;
}

def parse_name() -> *const char {
  let name = token.name;
  expect_token(TokenKind.Name);
  return name;
}

def parse_decl_enum_item() -> EnumItem {
  let pos = token.pos;
  let name = parse_name();
  let init: &Expr = NULL;
  if (match_token(TokenKind.Assign)) {
    init = parse_expr();
  }
  return (pos, name, init);
}

def parse_decl_enum(pos: SrcPos) -> &Decl {
  let name: *const char = NULL;
  if (is_token(TokenKind.Name)) {
    name = parse_name();
  }
  let type: &Typespec = NULL;
  if (match_token(TokenKind.Assign)) {
    type = parse_type();
  }
  expect_token(TokenKind.Lbrace);
  let items: *EnumItem = NULL;
  while (!is_token(TokenKind.Rbrace)) {
    let item = parse_decl_enum_item();
    buf_push(&items as &&void, &item, sizeof(item));
    if (!match_token(TokenKind.Comma)) {
      break;
    }
  }
  expect_token(TokenKind.Rbrace);
  return new_decl_enum(pos, name, type, (items, buf_len(items)));
}

def parse_decl_aggregate_item() -> AggregateItem {
  let pos = token.pos;
  if (match_keyword(struct_keyword)) {
    return {
      pos: pos,
      kind: AggregateItem.Subaggregate,
      subaggregate: parse_aggregate(AggregateKind.Struct, NULL, NULL),
    };
  } else if (match_keyword(union_keyword)) {
    return {
      pos: pos,
      kind: AggregateItem.Subaggregate,
      subaggregate: parse_aggregate(AggregateKind.Union, NULL, NULL),
    };
  } else {
    let names: **const char = NULL;
    let name = parse_name();
    buf_push(&names as &&void, &name, sizeof(name));
    while (match_token(TokenKind.Comma)) {
      name = parse_name();
      buf_push(&names as &&void, &name, sizeof(name));
    }
    expect_token(TokenKind.Colon);
    let type = parse_type();
    expect_token(TokenKind.Semicolon);
    return {
      pos: pos,
      kind: AggregateItem.Field,
      names: (names, buf_len(names)),
      type: type,
    };
  } if;
}

let enum_tag_names = <[]*const char>["kind"];
let enum_tag_name_interned = false;

def parse_aggregate(
  kind: AggregateKind, name: *const char, notes: &Slice<Note>,
) -> &Aggregate {
  let pos = token.pos;
  let items: *AggregateItem;
  let has_enum = false;
  let enum_union: &Aggregate;
  if (match_keyword(switch_keyword)) {
    if (kind != AggregateKind.Union) {
      fatal_error(pos, "Switch aggregate applies only to union");
    }
    items = parse_switch_union();
    has_enum = true;
  } else {
    expect_token(TokenKind.Lbrace);
    while (!is_token_eof() && !is_token(TokenKind.Rbrace)) {
      let item = parse_decl_aggregate_item();
      buf_push(&items as &&void, &item, sizeof(item));
      // See if it's a switch union.
      if (item.kind == AggregateItem.Subaggregate) {
        let sub = item.subaggregate;
        if (sub.kind == AggregateKind.Union && sub.items.length) {
          if (sub.items.items.kind_names.length) {
            // But only one subunion here is allowed.
            if (enum_union) {
              fatal_error(sub.pos, "Multiple switch unions in struct");
              return NULL;
            }
            enum_union = sub;
          } if;
        } if;
      } if;
    } while;
    expect_token(TokenKind.Rbrace);
    if (notes && get_note(notes, enum_keyword)) {
      has_enum = true;
    }
  } if;
  // Check for enum union conditions.
  // TODO Change conditions for enum union. Like has a switch union.
  // TODO Then default to enum union.
  if (name && (has_enum || enum_union)) {
    if (kind == AggregateKind.Union) {
      // This is the union. Contain it.
      enum_union = new_aggregate(pos, kind, (items, buf_len(items)));
      let subitem = AggregateItem {
        pos: pos,
        kind: AggregateItem.Subaggregate,
        subaggregate: enum_union,
      };
      // Wrap it in a struct.
      kind = AggregateKind.Struct;
      items = NULL;
      buf_push(&items as &&void, &subitem, sizeof(subitem));
    } else if (!enum_union) {
      enum_union = get_subunion((items, buf_len(items)));
      if (!enum_union) {
        fatal_error(pos, "No union for enum tag");
        return NULL;
      }
    }
    // Insert the new kind member to the struct at the top.
    // TODO Do we know up front to avoid this after the fact insertion?
    if (!enum_tag_name_interned) {
      enum_tag_names[0] = str_intern(enum_tag_names[0]);
      enum_tag_name_interned = true;
    }
    let tag_type_name = build_scoped_name(name, "Kind");
    let tag_item = AggregateItem {
      pos: pos,
      kind: AggregateItem.Field,
      names: (enum_tag_names, 1),
      type: new_typespec_name1(pos, tag_type_name),
    };
    buf_unshift(&items as &&void, &tag_item, sizeof(tag_item));
    // Create and attach union enum.
    // TODO Then use that elsewhere instead of having to recheck notes.
    build_enum_union_decl(enum_union, name);
  } if;
  let result = new_aggregate(pos, kind, (items, buf_len(items)));
  if (enum_union) {
    // Remember this at top for easy access later.
    result.union_enum_decl = enum_union.union_enum_decl;
  }
  return result;
}

let empty_names = <[]*const char>[""];

def parse_switch_union() -> *AggregateItem {
  expect_token(TokenKind.Lbrace);
  let items: *AggregateItem;
  while (!is_token_eof() && !is_token(TokenKind.Rbrace)) {
    let kind_names: **const char;
    let kind_name = parse_name();
    buf_push(&kind_names as &&void, &kind_name, sizeof(kind_name));
    // TODO Ellipse ranges.
    while (match_token(TokenKind.Comma)) {
      kind_name = parse_name();
      buf_push(&kind_names as &&void, &kind_name, sizeof(kind_name));
    }
    expect_token(TokenKind.Spear);
    let pos = token.pos;
    let item: AggregateItem;
    if (match_token(TokenKind.Colon)) {
      let type = parse_type();
      if (!(
        type.kind == Typespec.Name &&
        type.names.length == 1 &&
        type.names[0].name == void_name
      )) {
        fatal_error(token.pos, "Anonymous switch fields can only be void");
        return NULL;
      }
      expect_token(TokenKind.Semicolon);
      item = {
        pos: pos,
        kind: AggregateItem.Field,
        names: (empty_names, 1),
        type: type,
      };
    } else {
      item = parse_decl_aggregate_item();
      if (item.kind == AggregateItem.Field && item.names.length != 1) {
        fatal_error(item.pos, "Switch union field requires exactly one name");
        return NULL;
      }
    }
    item.kind_names = (kind_names, buf_len(kind_names));
    buf_push(&items as &&void, &item, sizeof(item));
  } while;
  expect_token(TokenKind.Rbrace);
  return items;
}

def build_enum_union_decl(enum_union: &Aggregate, decl_name: *const char) {
  // TODO Extract function.
  // Lowercase enum because we can't have a union member named enum, at least
  // not without escaping, so this uses a less likely collision, even if it's
  // a lowercase typename.
  let enum_type_name = build_scoped_name(decl_name, "Kind");
  // Figure out how many items we really have.
  let num_all_items: usize = 0;
  for &(enum_union.items) do(union_item) {
    let num_names = union_item.kind_names.length;
    if (!num_names) {
      // Non-switch enum union.
      if (union_item.kind != AggregateItem.Field) {
        fatal_error(
          union_item.pos, "Enum union item of %s not a field", decl_name,
        );
        return;
      }
      num_names = union_item.names.length;
    } if;
    num_all_items += num_names;
  } for;
  // Now build the enum items.
  let enum_items = xmalloc(num_all_items * sizeof(EnumItem)) as *EnumItem;
  let enum_item_index: usize = 0;
  for &(enum_union.items) do(union_item) {
    let names = union_item.kind_names;
    if (!names.length) {
      // Non switch.
      names = union_item.names;
    }
    for (names) do(name) {
      enum_items[enum_item_index++] = EnumItem {
        pos: union_item.pos, name: name, init: NULL,
      };
    } for;
  } for;
  // And add the new enum.
  let union_enum_decl = new_decl_enum(
    enum_union.pos, enum_type_name, NULL, (enum_items, num_all_items),
  );
  enum_union.union_enum_decl = union_enum_decl;
}

def parse_type_params() -> Slice<Decl> {
  let params: Slice<Decl>;
  if (match_token(TokenKind.Lt)) {
    do {
      let param_name = parse_name();
      let constraint: &Typespec;
      if (match_token(TokenKind.Colon)) {
        constraint = parse_type();
      }
      let param = Decl {
        kind: Decl.Typedef,
        pos: token.pos,
        is_generic: true,
        name: param_name,
        typedef_decl: {constraint: constraint},
        // Let val default to null.
      };
      buf_push(&params.items as &&void, &param, sizeof(param));
    } while (match_token(TokenKind.Comma));
    expect_token(TokenKind.Gt);
    params.length = buf_len(params.items);
  }
  return params;
}

def parse_decl_aggregate(
  pos: SrcPos, kind: Decl.Kind, notes: &Slice<Note>,
) -> &Decl {
  #assert(kind == Decl.Struct || kind == Decl.Union);
  let name = parse_name();
  let aggregate_kind =
    kind == Decl.Struct ? AggregateKind.Struct : AggregateKind.Union;
  let params = parse_type_params();
  let decl: &Decl;
  if (match_token(TokenKind.Semicolon)) {
    // TODO Need to allow switch here, too, for union switches.
    decl = new_decl_aggregate(
      pos, kind, name, params, new_aggregate(pos, aggregate_kind, {}),
    );
    decl.is_incomplete = true;
    return decl;
  } else {
    let aggregate = parse_aggregate(aggregate_kind, name, notes);
    // Map back again since this might have changed due to union switch.
    kind = aggregate.kind == AggregateKind.Struct ? Decl.Struct : Decl.Union;
    decl = new_decl_aggregate(pos, kind, name, params, aggregate);
  }
  if (params.length) {
    decl.is_generic = true;
  }
  return decl;
}

def parse_decl_var(pos: SrcPos) -> &Decl {
  let name = parse_name();
  if (match_token(TokenKind.Assign)) {
    let expr = parse_expr();
    expect_token(TokenKind.Semicolon);
    return new_decl_var(pos, name, NULL, expr);
  } else if (match_token(TokenKind.Colon)) {
    let type = parse_type();
    let expr: &Expr = NULL;
    if (match_token(TokenKind.Assign)) {
      expr = parse_expr();
    }
    expect_token(TokenKind.Semicolon);
    return new_decl_var(pos, name, type, expr);
  } else {
    fatal_error(token.pos, "Expected : or = after let, got %s", token_info());
    return NULL;
  }
}

def parse_decl_const(pos: SrcPos) -> &Decl {
  let name = parse_name();
  let type: &Typespec = NULL;
  if (match_token(TokenKind.Colon)) {
    type = parse_type();
  }
  expect_token(TokenKind.Assign);
  let expr = parse_expr();
  expect_token(TokenKind.Semicolon);
  return new_decl_const(pos, name, type, expr);
}

def parse_decl_typedef(pos: SrcPos) -> &Decl {
  let name = parse_name();
  expect_token(TokenKind.Assign);
  let type = parse_type();
  expect_token(TokenKind.Semicolon);
  return new_decl_typedef(pos, name, type);
}

def parse_decl_func_param(optional_types: bool) -> FuncParam {
  let pos = token.pos;
  let name = parse_name();
  // TODO Default args.
  let type: &Typespec;
  if (optional_types) {
    if (match_token(TokenKind.Colon)) {
      type = parse_type();
    }
  } else {
    expect_token(TokenKind.Colon);
    type = parse_type();
  }
  return (pos, name, type);
}

def parse_decl_func_params(
  has_varargs: &bool, optional_types: bool,
) -> Slice<FuncParam> {
  expect_token(TokenKind.Lparen);
  let params: *FuncParam = NULL;
  *has_varargs = false;
  while (!is_token(TokenKind.Rparen)) {
    if (match_token(TokenKind.Ellipsis)) {
      if (*has_varargs) {
        error(token.pos, "Multiple ellipsis in function declaration");
      }
      *has_varargs = true;
    } else {
      if (*has_varargs) {
        error(
          token.pos,
          "Ellipsis must be last parameter in function declaration",
        );
      }
      let param = parse_decl_func_param(optional_types);
      buf_push(&params as &&void, &param, sizeof(param));
    }
    if (!match_token(TokenKind.Comma)) {
      break;
    }
  } while;
  expect_token(TokenKind.Rparen);
  // Done.
  return (params, buf_len(params));
}

def parse_decl_func(pos: SrcPos) -> &Decl {
  let name = parse_name();
  let type_params = parse_type_params();
  let has_varargs: bool = false;
  let params = parse_decl_func_params(&has_varargs, false);
  let ret_type: &Typespec = NULL;
  if (match_token(TokenKind.Arrow)) {
    ret_type = parse_type();
  }
  let block: StmtList;
  let is_incomplete: bool;
  if (match_token(TokenKind.Semicolon)) {
    is_incomplete = true;
  } else {
    block = parse_stmt_block();
    is_incomplete = false;
  }
  let decl =
    new_decl_func(pos, name, type_params, params, ret_type, has_varargs, block);
  decl.is_incomplete = is_incomplete;
  if (type_params.length) {
    decl.is_generic = true;
  }
  return decl;
}

def parse_note_arg() -> NoteArg {
  let pos = token.pos;
  let expr = parse_expr();
  let name: *const char = NULL;
  if (match_token(TokenKind.Assign)) {
    if (expr.kind != Expr.Name) {
      fatal_error(token.pos, "Left of: operand = in note argument must be a name");
    }
    name = expr.name;
    expr = parse_expr();
  }
  return NoteArg {pos: pos, name: name, expr: expr};
}

def parse_note() -> Note {
  let pos = token.pos;
  let name: *const char;
  if (is_token(TokenKind.Keyword)) {
    name = token.name;
    next_token();
  } else {
    name = parse_name();
  }
  let args: *NoteArg = NULL;
  if (match_token(TokenKind.Lparen)) {
    let arg = parse_note_arg();
    buf_push(&args as &&void, &arg, sizeof(arg));
    while (match_token(TokenKind.Comma)) {
      arg = parse_note_arg();
      buf_push(&args as &&void, &arg, sizeof(arg));
    }
    expect_token(TokenKind.Rparen);
  } if;
  return new_note(pos, name, (args, buf_len(args)));
}

def parse_notes() -> Slice<Note> {
  let notes: *Note = NULL;
  while (match_token(TokenKind.At)) {
    let note = parse_note();
    buf_push(&notes as &&void, &note, sizeof(note));
  }
  return new_notes((notes, buf_len(notes)));
}

def parse_decl_note(pos: SrcPos) -> &Decl {
  return new_decl_note(pos, parse_note());
}

def parse_decl_import(pos: SrcPos) -> &Decl {
  let rename_name: *const char;
  let is_relative: bool;
  repeat:
  is_relative = false;
  if (match_token(TokenKind.Dot)) {
    is_relative = true;
  }
  let name = token.name;
  expect_token(TokenKind.Name);
  if (!is_relative && match_token(TokenKind.Assign)) {
    if (rename_name) {
      fatal_error(pos, "Only one import assignment is allowed");
    }
    rename_name = name;
    goto repeat;
  }
  let names: **const char = NULL;
  buf_push(&names as &&void, &name, sizeof(name));
  while (match_token(TokenKind.Dot)) {
    buf_push(&names as &&void, &token.name, sizeof(token.name));
    expect_token(TokenKind.Name);
  }
  let import_all = false;
  let items: *ImportItem;
  if (match_token(TokenKind.Lbrace)) {
    while (!is_token(TokenKind.Rbrace)) {
      if (match_token(TokenKind.Ellipsis)) {
        import_all = true;
      } else {
        let item_name = parse_name();
        if (match_token(TokenKind.Assign)) {
          let item = ImportItem {name: parse_name(), rename: item_name};
          buf_push(&items as &&void, &item, sizeof(item));
        } else {
          let item = ImportItem {name: item_name};
          buf_push(&items as &&void, &item, sizeof(item));
        }
        if (!match_token(TokenKind.Comma)) {
          break;
        }
      } if;
    } while;
    expect_token(TokenKind.Rbrace);
  } if;
  return new_decl_import(
    pos, rename_name, is_relative,
    (names, buf_len(names)), import_all, (items, buf_len(items)),
  );
}

def parse_decl_opt(notes: &Slice<Note>) -> &Decl {
  let pos = token.pos;
  if (match_keyword(enum_keyword)) {
    return parse_decl_enum(pos);
  } else if (match_keyword(struct_keyword)) {
    return parse_decl_aggregate(pos, Decl.Struct, notes);
  } else if (match_keyword(union_keyword)) {
    return parse_decl_aggregate(pos, Decl.Union, notes);
  } else if (match_keyword(const_keyword)) {
    return parse_decl_const(pos);
  } else if (match_keyword(typedef_keyword)) {
    return parse_decl_typedef(pos);
  } else if (match_keyword(def_keyword)) {
    return parse_decl_func(pos);
  } else if (match_keyword(let_keyword)) {
    return parse_decl_var(pos);
  } else if (match_keyword(import_keyword)) {
    return parse_decl_import(pos);
  } else if (match_token(TokenKind.Pound)) {
    return parse_decl_note(pos);
  } else {
    return NULL;
  }
}

def parse_decl() -> &Decl {
  let notes = parse_notes();
  let decl: &Decl = parse_decl_opt(&notes);
  if (!decl) {
    fatal_error(token.pos, "Expected declaration keyword, got %s", token_info());
  }
  decl.notes = notes;
  return decl;
}

def parse_decls() -> &Slice<&Decl> {
  let decls: *&Decl = NULL;
  while (!is_token(TokenKind.Eof)) {
    let decl = parse_decl();
    buf_push(&decls as &&void, &decl, sizeof(decl));
  }
  return new_decls((decls, buf_len(decls)));
}
